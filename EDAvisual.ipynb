{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 163 Final Project EDA Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== IMPORT LIBRARIES ==================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from fpdf import FPDF\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== SECTION 1: Load Dataset ==================\n",
    "file_path = \"C:/Users/sherv/Desktop/SP25/CS163/US_Accidents_March23.csv\"\n",
    "\n",
    "# Load dataset (keep original unchanged)\n",
    "df_original = pd.read_csv(file_path, low_memory=False)  \n",
    "df = df_original.copy()  # Work on a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== SECTION 2: Data Cleaning & Preprocessing ==================\n",
    "# Select relevant columns for analysis\n",
    "columns_to_keep = [\"Severity\", \"State\", \"Temperature(F)\", \"Humidity(%)\", \n",
    "                   \"Visibility(mi)\", \"Precipitation(in)\", \"Weather_Condition\", \n",
    "                   \"Pressure(in)\", \"Wind_Speed(mph)\"]\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Identify missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing Values Before Cleaning:\\n\", missing_values)\n",
    "\n",
    "# Drop missing values for simplicity\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Check the dataset size after dropping missing values\n",
    "print(f\"\\nRemaining Rows After Dropping NaN: {df.shape[0]:,}\")\n",
    "\n",
    "# ================== SECTION 3: Descriptive Statistics ==================\n",
    "# Ensure Pandas doesn't truncate output\n",
    "pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "pd.set_option(\"display.float_format\", \"{:,.2f}\".format)  # Avoid scientific notation\n",
    "\n",
    "# Summary statistics with better formatting\n",
    "summary_stats = df.describe().apply(lambda x: x.map(lambda y: f\"{y:,.2f}\"))  \n",
    "print(\"\\nSummary Statistics:\\n\", summary_stats.to_string())  # Force full output\n",
    "\n",
    "# Count the number of occurrences for each weather condition\n",
    "weather_condition_counts = df[\"Weather_Condition\"].value_counts()\n",
    "\n",
    "# Print full weather condition counts\n",
    "print(\"\\nWeather Condition Counts:\\n\", weather_condition_counts.to_string())  # Ensure full output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== SECTION 3: Descriptive Statistics ==================\n",
    "# Summary statistics with improved formatting\n",
    "summary_stats = df.describe().apply(lambda x: x.map(lambda y: f\"{y:,.2f}\"))  # Format numbers\n",
    "print(\"Summary Statistics:\\n\", summary_stats)\n",
    "\n",
    "# Format float output to avoid scientific notation\n",
    "pd.options.display.float_format = \"{:,.2f}\".format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average temperature during accidents is 61.4¬∞F, with humidity averaging 65.9%, and low precipitation values (mean = 0.0085 inches). The correlation matrix indicates weak correlations between severity and environmental factors (e.g., precipitation, temperature, humidity), confirming that these variables alone are not strong predictors of accident severity. However, humidity and visibility are negatively correlated (-0.41), meaning that higher humidity tends to reduce visibility, which could indirectly affect accident rates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== SECTION 4: Data Visualizations ==================\n",
    "# 1Ô∏è‚É£ Accident Severity Distribution Bar Chart\n",
    "# Count the number of accidents per severity level\n",
    "severity_counts = df[\"Severity\"].value_counts().sort_index()\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=severity_counts.index, y=severity_counts.values, palette=\"Blues\")\n",
    "\n",
    "# Annotate bars with exact counts\n",
    "for i, count in enumerate(severity_counts.values):\n",
    "    plt.text(i, count + 10000, f\"{count:,}\", ha=\"center\", fontsize=12)\n",
    "\n",
    "# Labels and title\n",
    "plt.title(\"Accident Severity Distribution\", fontsize=14)\n",
    "plt.xlabel(\"Severity Level\", fontsize=12)\n",
    "plt.ylabel(\"Number of Accidents\", fontsize=12)\n",
    "plt.xticks(ticks=[0, 1, 2, 3], labels=[\"1 (Low)\", \"2 (Moderate)\", \"3 (High)\", \"4 (Severe)\"])\n",
    "\n",
    "# Save and show\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Accident Severity Distribution bar chart reveals that Severity Level 2 (Moderate) accounts for the majority of accidents (~3.9 million), followed by Severity Level 3 (High) and a much smaller proportion of Severity Level 1 (Low) and Level 4 (Severe). This suggests that most accidents cause moderate disruptions to traffic flow rather than extreme consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Heatmap Insights:\n",
    "### Severity vs. Other Factors:\n",
    "\n",
    "Severity has weak correlations with all other features (-0.03 to 0.04), meaning accident severity isn't strongly dependent on temperature, humidity, visibility, or precipitation.\n",
    "Temperature(F) vs. Humidity(%) (-0.34)\n",
    "\n",
    "Moderate negative correlation: As temperature increases, humidity decreases (warmer air holds more moisture but reduces relative humidity).\n",
    "Humidity(%) vs. Visibility(mi) (-0.41)\n",
    "\n",
    "Moderate negative correlation: Higher humidity reduces visibility (likely due to fog, mist, or rain).\n",
    "Visibility(mi) vs. Precipitation(in) (-0.12)\n",
    "\n",
    "Slight negative correlation: Increased precipitation (rain/snow) slightly reduces visibility, but it‚Äôs not a strong effect.\n",
    "Precipitation(in) vs. Severity (0.02)\n",
    "\n",
    "No significant correlation: This means higher precipitation doesn't strongly impact accident severity.\n",
    "\n",
    "### What This Means for our EDA\n",
    "No strong predictor of accident severity among these weather variables.\n",
    "Humidity and visibility are more closely related, which makes sense since fog and precipitation affect visibility.\n",
    "You may need to explore other factors (e.g., road conditions, time of day, traffic volume) to better predict accident severity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ Scatter plot: Precipitation vs. Severity\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=df[\"Precipitation(in)\"], y=df[\"Severity\"], alpha=0.5, color=\"blue\")\n",
    "\n",
    "# Apply log scale to precipitation\n",
    "plt.xscale(\"log\")\n",
    "plt.xticks([0.1, 1, 5, 10, 20, 50], labels=[\"0.1\", \"1\", \"5\", \"10\", \"20\", \"50\"])  # Custom tick labels\n",
    "\n",
    "# Improve readability\n",
    "plt.title(\"Precipitation vs Severity (Log Scale)\", fontsize=14)\n",
    "plt.xlabel(\"Precipitation (inches, log scale)\", fontsize=12)\n",
    "plt.ylabel(\"Severity\", fontsize=12)\n",
    "\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Precipitation vs. Severity (Log Scale) scatter plot demonstrates that most accidents occur in low precipitation levels (<1 inch), with a small number of cases at higher precipitation levels. Since Severity is categorical (1-4), the log scale helps visualize how accident severity is distributed across different precipitation levels. The lack of a strong pattern suggests that precipitation alone does not strongly determine accident severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4Ô∏è‚É£ Bar Chart: Accidents by State\n",
    "# Count the number of accidents per state\n",
    "import matplotlib.ticker as mticker\n",
    "state_counts = df[\"State\"].value_counts().sort_values(ascending=False)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.barplot(x=state_counts.index, y=state_counts.values, palette=\"viridis\")\n",
    "\n",
    "# Improve Y-axis scale (increments of 100,000)\n",
    "plt.yticks(range(0, max(state_counts.values) + 100000, 100000))  # Set tick intervals\n",
    "\n",
    "# Format Y-axis labels with commas instead of scientific notation\n",
    "plt.gca().yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{int(x):,}'))\n",
    "\n",
    "# Rotate x-axis labels for readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add vertical labels on bars\n",
    "for i, count in enumerate(state_counts.values):\n",
    "    plt.text(i, count + 10000, f\"{count:,}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\n",
    "\n",
    "# Labels and Title\n",
    "plt.title(\"Accident Distribution by State\", fontsize=14)\n",
    "plt.xlabel(\"State\", fontsize=12)\n",
    "plt.ylabel(\"Number of Accidents\", fontsize=12)\n",
    "\n",
    "# Save and show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Accident Distribution by State bar chart highlights that California (CA), Florida (FL), and Texas (TX) have the highest number of recorded accidents, with California exceeding 1 million accidents. This trend suggests that states with high population density and urban traffic congestion tend to experience more accidents. Conversely, states like Vermont (VT), South Dakota (SD), and Wyoming (WY) have significantly lower accident counts, likely due to lower population density and fewer urban roadways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================== Data Cleaning ==================\n",
    "# Keep only relevant columns and drop missing values\n",
    "df_map = df_original[[\"Start_Lat\", \"Start_Lng\"]].dropna()\n",
    "\n",
    "# Convert DataFrame to a list of [lat, lon] for HeatMap\n",
    "heat_data = df_map.values.tolist()\n",
    "\n",
    "# ================== Create Folium Heatmap ==================\n",
    "# Initialize the map centered in the U.S.\n",
    "m = folium.Map(location=[37.8, -96], zoom_start=5, tiles=\"CartoDB Voyager\", attr=\"Stamen Terrain, OpenStreetMap\")\n",
    "\n",
    "\n",
    "# Add heatmap layer\n",
    "HeatMap(\n",
    "    heat_data, \n",
    "    radius=8,    # Adjust size of heat points\n",
    "    blur=4,      # Adjust blur effect\n",
    "    max_zoom=10  # Improve zoom visibility\n",
    ").add_to(m)\n",
    "\n",
    "# ================== Save and Display Map ==================\n",
    "# Save map to an HTML file\n",
    "m.save(\"US_Accident_Heatmap.html\")\n",
    "\n",
    "# Display map (if running in Jupyter Notebook)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Conclusion from initial EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, these findings suggest that urban density, traffic congestion, and other external factors likely have a more significant impact on accident frequency and severity than weather conditions alone. Further analysis could explore time-of-day trends, road conditions, and traffic volume to refine predictive insights. üöóüìä\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Visualizations #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_original\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import pandas as pd\n",
    "\n",
    "# === Sample the dataset to reduce clutter ===\n",
    "df_map = df_original[[\"Start_Lat\", \"Start_Lng\"]].dropna()\n",
    "\n",
    "# Adjust this to control how many points you show (e.g., 10,000 points)\n",
    "df_sampled = df_map.sample(n= 3000000, random_state=42)\n",
    "\n",
    "# Convert to list for heatmap\n",
    "heat_data = df_sampled.values.tolist()\n",
    "\n",
    "# === Create Heatmap ===\n",
    "m = folium.Map(location=[37.8, -96], zoom_start=5, tiles=\"CartoDB Voyager\", attr=\"Stamen Terrain, OpenStreetMap\")\n",
    "\n",
    "HeatMap(\n",
    "    heat_data,\n",
    "    radius=2,    # smaller radius = tighter points\n",
    "    blur=2,      # lower blur = sharper edges\n",
    "    max_zoom=10\n",
    ").add_to(m)\n",
    "\n",
    "m.save(\"US_Accident_Heatmap_Sampled.html\")\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Visualizations ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Weather-related columns\n",
    "weather_columns = [\n",
    "    \"Temperature(F)\",\n",
    "    \"Humidity(%)\",\n",
    "    \"Wind_Speed(mph)\",\n",
    "    \"Pressure(in)\",\n",
    "    \"Precipitation(in)\",\n",
    "    \"Visibility(mi)\"\n",
    "]\n",
    "\n",
    "# Columns to retain in final dataset\n",
    "columns_to_keep = [\"ID\", \"Severity\"] + weather_columns\n",
    "\n",
    "# Drop rows with missing weather data from the original dataframe\n",
    "df_weather_subset = df_original.dropna(subset=weather_columns)[columns_to_keep]\n",
    "\n",
    "# Save the result\n",
    "df_weather_subset.to_csv(\"US_Accidents_Weather_Only_With_ID.csv\", index=False)\n",
    "\n",
    "# Summary output\n",
    "print(f\"Original dataset size: {df_original.shape[0]} rows\")\n",
    "print(f\"Filtered dataset size (weather + severity + ID only): {df_weather_subset.shape[0]} rows\")\n",
    "print(f\"Columns in new dataset: {df_weather_subset.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define expanded regex pattern to capture I-5, I 5, US-101, US 101, Hwy, etc.\n",
    "highway_pattern = r'\\b(I[-\\s]?\\d+|US[-\\s]?\\d+|Hwy|HWY|highway)\\b'\n",
    "\n",
    "# Create a new 'highway' column in df_original\n",
    "df_original['highway'] = df_original['Description'].str.contains(highway_pattern, flags=re.IGNORECASE, na=False)\n",
    "\n",
    "# Create a filtered DataFrame containing only highway-related accidents\n",
    "df_highway_only = df_original[df_original['highway'] == True]\n",
    "\n",
    "# Save the filtered data to CSV (optional)\n",
    "df_highway_only.to_csv(\"Filtered_Highway_Accidents.csv\", index=False)\n",
    "\n",
    "# Preview the result\n",
    "print(f\"‚úÖ Rows with highway mentions: {df_highway_only.shape[0]}\")\n",
    "print(df_highway_only[['ID', 'Description', 'highway']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Use non-capturing groups to avoid warning\n",
    "highway_pattern = r'\\b(?:I[-\\s]?\\d+|US[-\\s]?\\d+|Hwy|HWY|highway)\\b'\n",
    "\n",
    "# Add 'highway' column with True/False values based on Description content\n",
    "df_original['highway'] = df_original['Description'].str.contains(highway_pattern, flags=re.IGNORECASE, na=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind, f_oneway, pearsonr\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"US_Accidents_With_Highway_Flag.csv\")\n",
    "\n",
    "# Define numeric columns to include (excluding ID and categorical features)\n",
    "numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "\n",
    "# Drop 'Severity' from the list since it's used as the label\n",
    "if \"Severity\" in numeric_cols:\n",
    "    numeric_cols.remove(\"Severity\")\n",
    "\n",
    "# Drop rows with missing values in any selected numeric column\n",
    "df = df[[\"Severity\"] + numeric_cols].dropna()\n",
    "\n",
    "# Group severity into binary for T-test: Low (1-2), High (3-4)\n",
    "df[\"Severity_Group\"] = df[\"Severity\"].apply(lambda x: \"Low\" if x <= 2 else \"High\")\n",
    "\n",
    "# Prepare results\n",
    "results = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    # T-Test between Low vs High severity\n",
    "    group_low = df[df[\"Severity_Group\"] == \"Low\"][col]\n",
    "    group_high = df[df[\"Severity_Group\"] == \"High\"][col]\n",
    "    t_stat, t_pval = ttest_ind(group_low, group_high, equal_var=False)\n",
    "\n",
    "    # ANOVA across all 4 severity levels\n",
    "    groups = [df[df[\"Severity\"] == s][col] for s in sorted(df[\"Severity\"].unique())]\n",
    "    a_stat, a_pval = f_oneway(*groups)\n",
    "\n",
    "    # Pearson correlation with Severity\n",
    "    corr, corr_pval = pearsonr(df[col], df[\"Severity\"])\n",
    "\n",
    "    # Append result\n",
    "    results.append({\n",
    "        \"Feature\": col,\n",
    "        \"T-Test p-value\": round(t_pval, 5),\n",
    "        \"ANOVA p-value\": round(a_pval, 5),\n",
    "        \"Correlation (r)\": round(corr, 3),\n",
    "        \"Correlation p-value\": round(corr_pval, 5)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nStatistical Test Results for All Numeric Features:\\n\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Step 1: Load your dataset ===\n",
    "df = pd.read_csv(\"US_Accidents_With_Highway_Flag.csv\")\n",
    "\n",
    "# === Step 2: Define features and target ===\n",
    "features = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Wind_Speed(mph)\",\n",
    "    \"Pressure(in)\", \"Precipitation(in)\", \"Visibility(mi)\", \"highway\"\n",
    "]\n",
    "\n",
    "# Drop rows with missing values in selected columns\n",
    "df = df[[\"Severity\"] + features].dropna()\n",
    "df[\"highway\"] = df[\"highway\"].astype(int)\n",
    "\n",
    "# Binary classification target: 0 = Low (1‚Äì2), 1 = High (3‚Äì4)\n",
    "df[\"Severity_Binary\"] = df[\"Severity\"].apply(lambda x: 0 if x <= 2 else 1)\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"Severity_Binary\"]\n",
    "\n",
    "# === Step 3: Scale numeric features ===\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[features[:-1]] = scaler.fit_transform(X_scaled[features[:-1]])\n",
    "\n",
    "# === Step 4: Train-test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === Step 5: Train Random Forest with class balancing ===\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight='balanced', random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# === Step 6: Predict and evaluate ===\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Low Severity\", \"High Severity\"]))\n",
    "\n",
    "# Show confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(rf_model, X_test, y_test, cmap=\"Blues\")\n",
    "plt.title(\"Random Forest - Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Load dataset ===\n",
    "df = pd.read_csv(\"US_Accidents_With_Highway_Flag.csv\")\n",
    "\n",
    "# === Define features and target ===\n",
    "features = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Wind_Speed(mph)\",\n",
    "    \"Pressure(in)\", \"Precipitation(in)\", \"Visibility(mi)\", \"highway\"\n",
    "]\n",
    "\n",
    "df = df[[\"Severity\"] + features].dropna()\n",
    "df[\"highway\"] = df[\"highway\"].astype(int)\n",
    "df[\"Severity_Binary\"] = df[\"Severity\"].apply(lambda x: 0 if x <= 2 else 1)\n",
    "\n",
    "# === Split first, then balance ===\n",
    "X = df[features]\n",
    "y = df[\"Severity_Binary\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === Combine and balance training data (undersampling) ===\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "majority = train_df[train_df[\"Severity_Binary\"] == 0]\n",
    "minority = train_df[train_df[\"Severity_Binary\"] == 1]\n",
    "majority_downsampled = resample(majority, replace=False, n_samples=len(minority), random_state=42)\n",
    "train_balanced = pd.concat([majority_downsampled, minority])\n",
    "\n",
    "# === Balance the test set similarly ===\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "majority_test = test_df[test_df[\"Severity_Binary\"] == 0]\n",
    "minority_test = test_df[test_df[\"Severity_Binary\"] == 1]\n",
    "majority_test_down = resample(majority_test, replace=False, n_samples=len(minority_test), random_state=42)\n",
    "test_balanced = pd.concat([majority_test_down, minority_test])\n",
    "\n",
    "# === Scale and split X/y ===\n",
    "scaler = StandardScaler()\n",
    "X_train_bal = scaler.fit_transform(train_balanced[features])\n",
    "y_train_bal = train_balanced[\"Severity_Binary\"]\n",
    "X_test_bal = scaler.transform(test_balanced[features])\n",
    "y_test_bal = test_balanced[\"Severity_Binary\"]\n",
    "\n",
    "# === Train Random Forest ===\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# === Evaluate ===\n",
    "y_pred = rf.predict(X_test_bal)\n",
    "\n",
    "print(\"Classification Report (Balanced Train & Test):\\n\")\n",
    "print(classification_report(y_test_bal, y_pred, target_names=[\"Low\", \"High\"]))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(rf, X_test_bal, y_test_bal, cmap=\"Blues\")\n",
    "plt.title(\"Random Forest - Balanced Train/Test\")\n",
    "plt.show()\n",
    "\n",
    "# === Feature Importance Bar Plot ===\n",
    "importances = rf.feature_importances_\n",
    "feature_names = features\n",
    "\n",
    "feat_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n",
    "feat_df = feat_df.sort_values(by=\"Importance\", ascending=True)\n",
    "\n",
    "feat_df.plot(kind=\"barh\", x=\"Feature\", y=\"Importance\", legend=False, color=\"skyblue\")\n",
    "plt.title(\"Feature Importance (Random Forest)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# === Feature importance data (assuming you've already trained your model) ===\n",
    "importances = rf.feature_importances_\n",
    "feature_names = features\n",
    "\n",
    "feat_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n",
    "feat_df = feat_df.sort_values(by=\"Importance\", ascending=True)\n",
    "\n",
    "# === Generate color palette based on number of features ===\n",
    "colors = sns.color_palette(\"coolwarm\", len(feat_df))\n",
    "\n",
    "# === Plot feature importance with custom colors ===\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(feat_df[\"Feature\"], feat_df[\"Importance\"], color=colors)\n",
    "plt.title(\"Feature Importance (Random Forest)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Load dataset ===\n",
    "df = pd.read_csv(\"US_Accidents_With_Highway_Flag.csv\")\n",
    "\n",
    "# === Define features and target ===\n",
    "features = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Wind_Speed(mph)\",\n",
    "    \"Pressure(in)\", \"Precipitation(in)\", \"Visibility(mi)\", \"highway\"\n",
    "]\n",
    "\n",
    "df = df[[\"Severity\"] + features].dropna()\n",
    "df[\"highway\"] = df[\"highway\"].astype(int)\n",
    "df[\"Severity_Binary\"] = df[\"Severity\"].apply(lambda x: 0 if x <= 2 else 1)\n",
    "\n",
    "# === Split first, then balance ===\n",
    "X = df[features]\n",
    "y = df[\"Severity_Binary\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === Combine and balance training data (undersampling) ===\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Separate classes\n",
    "majority = train_df[train_df[\"Severity_Binary\"] == 0]\n",
    "minority = train_df[train_df[\"Severity_Binary\"] == 1]\n",
    "\n",
    "# Undersample majority class to match minority\n",
    "majority_downsampled = resample(majority, replace=False, n_samples=len(minority), random_state=42)\n",
    "train_balanced = pd.concat([majority_downsampled, minority])\n",
    "\n",
    "# === Balance the test set the same way ===\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "majority_test = test_df[test_df[\"Severity_Binary\"] == 0]\n",
    "minority_test = test_df[test_df[\"Severity_Binary\"] == 1]\n",
    "majority_test_down = resample(majority_test, replace=False, n_samples=len(minority_test), random_state=42)\n",
    "test_balanced = pd.concat([majority_test_down, minority_test])\n",
    "\n",
    "# === Scale and split X/y ===\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_bal = scaler.fit_transform(train_balanced[features])\n",
    "y_train_bal = train_balanced[\"Severity_Binary\"]\n",
    "\n",
    "X_test_bal = scaler.transform(test_balanced[features])\n",
    "y_test_bal = test_balanced[\"Severity_Binary\"]\n",
    "\n",
    "# === Train Random Forest ===\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# === Evaluate ===\n",
    "y_pred = rf.predict(X_test_bal)\n",
    "\n",
    "print(\"Classification Report (Balanced Train & Test):\\n\")\n",
    "print(classification_report(y_test_bal, y_pred, target_names=[\"Low\", \"High\"]))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(rf, X_test_bal, y_test_bal, cmap=\"Blues\")\n",
    "plt.title(\"Random Forest - Balanced Train/Test\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from scipy.stats import ttest_ind, f_oneway, pearsonr\n",
    "\n",
    "# === Load your dataset ===\n",
    "df = pd.read_csv(\"US_Accidents_With_Highway_Flag.csv\")\n",
    "\n",
    "# === Define numeric columns ===\n",
    "numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col != \"Severity\"]\n",
    "\n",
    "# === Drop missing values for selected numeric columns + Severity ===\n",
    "df = df[[\"Severity\"] + numeric_cols].dropna()\n",
    "\n",
    "# === Binary group: Low (1‚Äì2) vs High (3‚Äì4) ===\n",
    "df[\"Severity_Binary\"] = df[\"Severity\"].apply(lambda x: 0 if x <= 2 else 1)\n",
    "\n",
    "# === Create a balanced small subset (undersample majority class) ===\n",
    "low = df[df[\"Severity_Binary\"] == 0]\n",
    "high = df[df[\"Severity_Binary\"] == 1]\n",
    "sample_size = min(len(low), len(high), 10000)  # you can adjust 10000\n",
    "\n",
    "low_sample = resample(low, replace=False, n_samples=sample_size, random_state=42)\n",
    "high_sample = resample(high, replace=False, n_samples=sample_size, random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([low_sample, high_sample])\n",
    "\n",
    "# === Add Severity Group for readability (for T-test) ===\n",
    "df_balanced[\"Severity_Group\"] = df_balanced[\"Severity\"].apply(lambda x: \"Low\" if x <= 2 else \"High\")\n",
    "\n",
    "# === Run statistical tests ===\n",
    "results = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    group_low = df_balanced[df_balanced[\"Severity_Group\"] == \"Low\"][col]\n",
    "    group_high = df_balanced[df_balanced[\"Severity_Group\"] == \"High\"][col]\n",
    "    t_stat, t_pval = ttest_ind(group_low, group_high, equal_var=False)\n",
    "    \n",
    "    groups_anova = [df_balanced[df_balanced[\"Severity\"] == s][col] for s in sorted(df_balanced[\"Severity\"].unique())]\n",
    "    a_stat, a_pval = f_oneway(*groups_anova)\n",
    "    \n",
    "    corr, corr_pval = pearsonr(df_balanced[col], df_balanced[\"Severity\"])\n",
    "\n",
    "    results.append({\n",
    "        \"Feature\": col,\n",
    "        \"T-Test p-value\": round(t_pval, 5),\n",
    "        \"ANOVA p-value\": round(a_pval, 5),\n",
    "        \"Correlation (r)\": round(corr, 3),\n",
    "        \"Correlation p-value\": round(corr_pval, 5)\n",
    "    })\n",
    "\n",
    "# === Display results ===\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nStatistical Test Results (Balanced Subset):\\n\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"US_Accidents_With_Highway_Flag.csv\")\n",
    "\n",
    "# Define categorical/boolean columns (you can expand this list)\n",
    "categorical_cols = [\n",
    "    \"Amenity\", \"Bump\", \"Crossing\", \"Junction\",\n",
    "    \"No_Exit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\",\n",
    "    \"Traffic_Calming\", \"Traffic_Signal\", \"Turning_Loop\"\n",
    "]\n",
    "\n",
    "# Clean the data\n",
    "df = df[[\"Severity\"] + categorical_cols].dropna()\n",
    "df[\"Severity_Binary\"] = df[\"Severity\"].apply(lambda x: 0 if x <= 2 else 1)\n",
    "\n",
    "# Run chi-square tests\n",
    "results = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    contingency = pd.crosstab(df[col], df[\"Severity_Binary\"])\n",
    "    chi2, p, dof, _ = chi2_contingency(contingency)\n",
    "    results.append({\n",
    "        \"Feature\": col,\n",
    "        \"Chi2 Stat\": round(chi2, 2),\n",
    "        \"P-Value\": round(p, 5)\n",
    "    })\n",
    "\n",
    "# Show results\n",
    "chi2_df = pd.DataFrame(results).sort_values(by=\"P-Value\")\n",
    "print(\"\\nChi-Square Test Results (Categorical vs Severity):\\n\")\n",
    "print(chi2_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "feat_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n",
    "feat_df = feat_df.sort_values(by=\"Importance\", ascending=True)\n",
    "\n",
    "feat_df.plot(kind=\"barh\", x=\"Feature\", y=\"Importance\", legend=False, color=\"skyblue\")\n",
    "plt.title(\"Feature Importance (Random Forest)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Example chi-square results\n",
    "data = {\n",
    "    \"Feature\": [\n",
    "        \"Amenity\", \"Bump\", \"Crossing\", \"Junction\", \"No_Exit\", \"Railway\", \"Roundabout\",\n",
    "        \"Station\", \"Stop\", \"Traffic_Calming\", \"Traffic_Signal\", \"Turning_Loop\"\n",
    "    ],\n",
    "    \"Chi2 Stat\": [\n",
    "        10790.17, 219.18, 96950.15, 21154.17, 836.95, 808.37, 35.00,\n",
    "        18902.32, 24656.23, 233.59, 84836.52, 0.00\n",
    "    ],\n",
    "    \"P-Value\": [\n",
    "        0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
    "        0.00, 0.00, 0.00, 0.00, 1.00\n",
    "    ]\n",
    "}\n",
    "\n",
    "chi_df = pd.DataFrame(data)\n",
    "chi_df = chi_df.sort_values(by=\"Chi2 Stat\", ascending=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(chi_df[\"Feature\"], chi_df[\"Chi2 Stat\"], color=\"coral\")\n",
    "plt.title(\"Chi-Square Test Statistic by Feature\")\n",
    "plt.xlabel(\"Chi2 Statistic (Strength of Association with Severity)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1 ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
